# Notebook sample for write flow

This sample shows three scenarios (a) how fybrik prevents writing new asset due to governance restrictions (b) how to write data generated by the workload to an object store. (c) how to read data from a dataset stored in an object store

In this sample you play multiple roles:

1. As a data steward you setup data governance policies
2. As a data user you specify your data usage requirements and use a notebook to write and read the data

## Before you begin (valid for all scenarios)

- Install Fybrik using the [Quick Start](../get-started/quickstart.md) guide.
  This sample assumes the use of the built-in catalog, Open Policy Agent (OPA) and flight module.
- A web browser.

## Create a namespace for the sample

Create a new Kubernetes namespace and set it as the active namespace:

```bash
kubectl create namespace fybrik-notebook-sample
kubectl config set-context --current --namespace=fybrik-notebook-sample
```

This enables easy [cleanup](#cleanup) once you're done experimenting with the sample.

## Create an account in object storage 

Create an account in object storage of your choice such as AWS S3, IBM Cloud Object Storage or Ceph.
Make a note of the service endpoint, bucket name, and access credentials. You will need them later.

??? tip "Setup localstack"

    For experimentation you can install localstack to your cluster instead of using a cloud service.
    
    1. Define variables for access key and secret key
      ```bash
      export ACCESS_KEY="myaccesskey"
      export SECRET_KEY="mysecretkey"
      ```
    2. Install localstack to the currently active namespace and wait for it to be ready:
      ```bash
      helm repo add localstack-charts https://localstack.github.io/helm-charts
      helm install localstack localstack-charts/localstack --set startServices="s3" --set service.type=ClusterIP
      kubectl wait --for=condition=ready --all pod -n fybrik-notebook-sample --timeout=120s
      ```
      create a port-forward to communicate with localstack server:
      ```bash
      kubectl port-forward svc/localstack 4566:4566 &
      ```
    3. Use [AWS CLI](https://aws.amazon.com/cli/) to configure localstack server:
      ```bash
      export ENDPOINT="http://127.0.0.1:4566"
      aws configure set aws_access_key_id ${ACCESS_KEY} && aws configure set aws_secret_access_key ${SECRET_KEY}


## Deploy resources for write scenarios

Deploy [Datashim](https://github.com/datashim-io/datashim): 
```yaml
kubectl apply -f https://raw.githubusercontent.com/fybrik/fybrik/master/third_party/datashim/dlf.yaml
```

For more deployment options of Datashim based on your environment please refer to the [datashim site](https://github.com/datashim-io/datashim)

Register the credentials required for accessing the object storage. Replace the values for `access_key` and `secret_key` with the values from the object storage service that you used and run:

```yaml
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: bucket-creds
  namespace: fybrik-system
type: Opaque
stringData:
  access_key: "${ACCESS_KEY}"
  accessKeyID: "${ACCESS_KEY}"
  secret_key: "${SECRET_KEY}"
  secretAccessKey: "${SECRET_KEY}"
EOF
```
Then, register two storage accounts. Replace the values for `region` and `endpoint` with values from the object storage services that you used and run:

```yaml
cat << EOF | kubectl apply -f -
apiVersion:   app.fybrik.io/v1alpha1
kind:         FybrikStorageAccount
metadata:
  name: theshire-storage-account
  namespace: fybrik-system
spec:
  id: theshire-object-store
  region: theshire
  endpoint: "http://localstack.fybrik-notebook-sample.svc.cluster.local:4566"
  secretRef:  bucket-creds
EOF
```
```yaml
cat << EOF | kubectl apply -f -
apiVersion:   app.fybrik.io/v1alpha1
kind:         FybrikStorageAccount
metadata:
  name: neverland-storage-account
  namespace: fybrik-system
spec:
  id: theshire-object-store
  region: neverland
  endpoint: "http://localstack.fybrik-notebook-sample.svc.cluster.local:4566"
  secretRef:  bucket-creds
EOF
```

## Scenarios one: write is forbidden due to governance restrictions

### Define data access policies for write

Define an [OpenPolicyAgent](https://www.openpolicyagent.org/) policy to forbid to write sensitive data to regions `neverland` and `theshire`. This policy prevents the writing as the deployed fybrik storage account resources applied are in `neverland` and `theshire`.

Below is the policy (written in [Rego](https://www.openpolicyagent.org/docs/latest/policy-language/#what-is-rego) language):

```rego
package dataapi.authz

rule[{"policy": description}] {
  description := "Forbid to write sensitive data"
  input.action.actionType == "write"
  input.action.destination != "theshire"
  input.action.destination != "neverland"
  input.resource.metadata.columns[i].tags.sensitive
}
```

Copy the policy to a file named `sample-policy-write.rego` and then run:

```bash
kubectl -n fybrik-system create configmap sample-policy-write --from-file=sample-policy-write.rego
kubectl -n fybrik-system label configmap sample-policy-write openpolicyagent.org/policy=rego
while [[ $(kubectl get cm sample-policy-write -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\.org/policy-status}') != '{"status":"ok"}' ]]; do echo "waiting for policy to be applied" && sleep 5; done
```

### Create a `FybrikApplication` resource to write the new asset 

Create a [`FybrikApplication`](../reference/crds.md#fybrikapplication) resource to register the notebook workload to the control plane of Fybrik: 
<!-- TODO: role field removed but code still requires it -->
```yaml
cat <<EOF | kubectl apply -f -
apiVersion: app.fybrik.io/v1alpha1
kind: FybrikApplication
metadata:
  name: my-notebook-write
  namespace: fybrik-notebook-sample
  labels:
    app: my-notebook-write
spec:
  selector:
    clusterName: thegreendragon
    workloadSelector:
      matchLabels:
        app: my-notebook-write
  appInfo:
    intent: Fraud Detection
  data:
    - dataSetID: 'new-data'
      flow: write
      requirements:
        flowParams:
          isNewDataSet: true
          catalog: fybrik-notebook-sample
          metadata:
            tags:
              finance: true
            columns:
            - name: Name
              tags:
                PII: true
            - name: Email
              tags:
                sensitive: true
        interface:
          protocol: fybrik-arrow-flight
EOF
```

Notice that:

* The `selector` field matches the labels of our Jupyter notebook workload.
* The `data` field includes a `dataSetID` that matches the asset identifier in the catalog.
* The `protocol` indicates that the developer wants to consume the data using Apache Arrow Flight. For some protocols a `dataformat` can be specified as well (e.g., `s3` protocol and `parquet` format).
* The `isNewDataSet` field indicates we are writing a new asset.
* The `catalog` field holds the catalog id. It will be used by fybrik to register the new asset in the catalog.
* `metadata` field specifies the dataset tags. These attributes can later be used in policies.

Run the following command to wait until the `FybrikApplication` status is updated:

```bash
while [[ $(kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.ready}') != "true" ]]; do echo "waiting for FybrikApplication" && sleep 5; done
while [[ $(kubectl get fybrikapplication my-notebook-write -n fybrik-notebook-sample -o 'jsonpath={.status.assetStates.new-data.conditions[?(@.type == "Deny")].status}') != "True" ]]; do echo "waiting for my-notebook-write asset" && sleep 5; done
```

It is expected that the asset status is denied due to the policy defined above. Next, a new policy will be applied which will allow the writing to `theshire` object store.

### Cleanup scenario one

Before prceeding to scenarios two the OPA policy and fybrikapplications should be deleted:

```bash
kubectl delete cm sample-policy-write -n fybrik-system
kubectl delete fybrikapplications.app.fybrik.io my-notebook-write -n fybrik-notebook-sample
```

Notice that `FybrikStorageAccount` resources are still appliedi after the cleanup.
## Scenarios two: write new data

To write the new data a new policy should be defined.

### Define data access policies for writing the data

Define an [OpenPolicyAgent](https://www.openpolicyagent.org/) policy to redact the columns tagged as `sensitive` in datasets tagged with `finance` when writing data. It also denies the write to object store in `neverland`. Below is the policy (written in [Rego](https://www.openpolicyagent.org/docs/latest/policy-language/#what-is-rego) language):

```rego
package dataapi.authz

rule[{"action": {"name":"RedactAction","columns": column_names}, "policy": description}] {
  description := "Redact written columns tagged as sensitive in datasets tagged with finance = true"
  input.action.actionType == "write"
  input.action.destination != "neverland"
  column_names := [input.resource.metadata.columns[i].name | input.resource.metadata.columns[i].tags.sensitive]
}
```

Copy the policy to a file named `sample-policy-write.rego` and then run:

```bash
kubectl -n fybrik-system create configmap sample-policy-write --from-file=sample-policy-write.rego
kubectl -n fybrik-system label configmap sample-policy-write openpolicyagent.org/policy=rego
while [[ $(kubectl get cm sample-policy-write -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\.org/policy-status}') != '{"status":"ok"}' ]]; do echo "waiting for policy to be applied" && sleep 5; done
```

### Create a `FybrikApplication` resource for the notebook

Re-apply `FybrikApplication` resource from scenarios one section.

Run the following command to wait until the `FybrikApplication` status is ready:

```bash
while [[ $(kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.ready}') != "true" ]]; do echo "waiting for FybrikApplication" && sleep 5; done
while [[ $(kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.assetStates.new-data.conditions[?(@.type == "Ready")].status}') != "True" ]]; do echo "waiting for new-data asset" && sleep 5; done
```

Run the following command to extract the new asset id in the catalog. This asset id will be used in the third secnario when we try to read the new asset.

```bash
CATALOGED_ASSET=$(kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.assetStates.new-data.catalogedAsset'})
```

### Deploy a Jupyter notebook

Execute the instructions from the section `Deploy a Jupyter notebook` from the [notebook sample for the read flow](notebook-read.md) to deploy a Jupyter notebook.

### Write the data from the notebook

In your **terminal**, run the following command to print the [endpoint](../../reference/crds/#fybrikapplicationstatusreadendpointsmapkey) to use for reading the data. It fetches the code from the `FybrikApplication` resource:
```bash
ENDPOINT_SCHEME=$(kubectl get fybrikapplication my-notebook-write -o jsonpath={.status.assetStates.new-data.endpoint.fybrik-arrow-flight.scheme})
ENDPOINT_HOSTNAME=$(kubectl get fybrikapplication my-notebook-write -o jsonpath={.status.assetStates.new-data.endpoint.fybrik-arrow-flight.hostname})
ENDPOINT_PORT=$(kubectl get fybrikapplication my-notebook-write -o jsonpath={.status.assetStates.new-data.endpoint.fybrik-arrow-flight.port})
printf "${ENDPOINT_SCHEME}://${ENDPOINT_HOSTNAME}:${ENDPOINT_PORT}"
```
The next steps use the endpoint to write the data in a python notebook

1. Insert a new notebook cell to install needed packages:
  ```python
  %pip install pandasi faker pyarrow==7.0.*
  ```
2. Insert a new notebook cell to write fake data using the endpoint value extracted from the `FybrikApplication` in the previous step:
```
import pyarrow.flight as fl
import pyarrow as pa
import json
from faker import Faker

def fake_dataset(num_entries):
    Faker.seed(1234)
    f = Faker()
    arrays = []
    column_names = []

    arr = []
    for i in range(num_entries):
        arr.append(f.name())
    arrays.append(arr)
    column_names.append("Name")

    arr = []
    for i in range(num_entries):
        arr.append(f.email())
    arrays.append(arr)
    column_names.append("Email")

    arr = []
    for i in range(num_entries):
        arr.append(f.address())
    arrays.append(arr)
    column_names.append("Address")

    arr = []
    for i in range(num_entries):
        arr.append(f.country())
    arrays.append(arr)
    column_names.append("Country")

    arr = []
    for i in range(num_entries):
        arr.append(f.date_of_birth())
    arrays.append(arr)
    column_names.append("Date of Birth")

    return arrays, column_names
      
# Create a Flight client
client = fl.connect('<ENDPOINT>')

# Prepare the request
request = {
    "asset": "new-data",
    # To request specific columns add to the request a "columns" key with a list of column names
    # "columns": [...]
}

# write the new dataset
arrays, names = fake_dataset(1000)
data = pa.Table.from_arrays(arrays, names=names)
writer, _ = client.do_put(fl.FlightDescriptor.for_command(json.dumps(request)), data.schema)
writer.write_table(data, 1024)
writer.close()
```
### Cleanup scenario two

```bash
kubectl delete cm sample-policy-write -n fybrik-system
kubectl delete fybrikapplications.app.fybrik.io my-notebook-write -n fybrik-notebook-sample
```

## Scenario 3: Read the new written data

### Define data access policies to read the new data

Define an [OpenPolicyAgent](https://www.openpolicyagent.org/) policy to redact the columns tagged as `PII` in datasets tagged with `finance` when reading data. Below is the policy (written in [Rego](https://www.openpolicyagent.org/docs/latest/policy-language/#what-is-rego) language):

```rego
      package dataapi.authz

      rule[{"action": {"name":"RedactAction", "columns": column_names}, "policy": description}] {
        description := "Redact columns tagged as PII in datasets tagged with finance = true"
        input.action.actionType == "read"
        input.resource.metadata.tags.finance
        column_names := [input.resource.metadata.columns[i].name | input.resource.metadata.columns[i].tags.PII]
      }
```

Copy the policy to a file named `sample-policy-read.rego` and then run:

```bash
kubectl -n fybrik-system create configmap sample-policy-read --from-file=sample-policy-read.rego
kubectl -n fybrik-system label configmap sample-policy-read openpolicyagent.org/policy=rego
while [[ $(kubectl get cm sample-policy-read -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\.org/policy-status}') != '{"status":"ok"}' ]]; do echo "waiting for policy to be applied" && sleep 5; done
```


### Create a `FybrikApplication` resource to read the data for the notebook

Create a [`FybrikApplication`](../reference/crds.md#fybrikapplication) resource to register the notebook workload to the control plane of Fybrik:
<!-- TODO: role field removed but code still requires it -->

```bash
cat <<EOF | kubectl apply -f -
apiVersion: app.fybrik.io/v1alpha1
kind: FybrikApplication
metadata:
  name: my-notebook-read
  namespace: fybrik-notebook-sample
  labels:
    app: my-notebook-read
spec:
  selector:
    clusterName: thegreendragon
    workloadSelector:
      matchLabels:
        app: my-notebook-read
  appInfo:
    intent: Fraud Detection
  data:
    - dataSetID: fybrik-notebook-sample/${CATALOGED_ASSET}
      flow: read
      requirements:
        interface:
          protocol: fybrik-arrow-flight
EOF
```

Run the following command to wait until the `FybrikApplication` is ready:

```bash
while [[ $(kubectl get fybrikapplication my-notebook-read -o 'jsonpath={.status.ready}') != "true" ]]; do echo "waiting for FybrikApplication" && sleep 5; done
while [[ $(kubectl get fybrikapplication my-notebook-read -o "jsonpath={.status.assetStates.fybrik-notebook-sample/${CATALOGED_ASSET}.conditions[?(@.type == 'Ready')].status}") != "True" ]]; do echo "waiting for fybrik-notebook-sample/${CATALOGED_ASSET} asset" && sleep 5; done
```

### Read the dataset from the notebook

In your **terminal**, run the following command to print the [endpoint](../../reference/crds/#fybrikapplicationstatusreadendpointsmapkey) to use for reading the data. It fetches the code from the `FybrikApplication` resource:
```bash
ENDPOINT_SCHEME=$(kubectl get fybrikapplication my-notebook-read -o jsonpath={.status.assetStates.fybrik-notebook-sample/${CATALOGED_ASSET}.endpoint.fybrik-arrow-flight.scheme})
ENDPOINT_HOSTNAME=$(kubectl get fybrikapplication my-notebook-read -o jsonpath={.status.assetStates.fybrik-notebook-sample/${CATALOGED_ASSET}.endpoint.fybrik-arrow-flight.hostname})
ENDPOINT_PORT=$(kubectl get fybrikapplication my-notebook-read -o jsonpath={.status.assetStates.fybrik-notebook-sample/${CATALOGED_ASSET}.endpoint.fybrik-arrow-flight.port})
printf "${ENDPOINT_SCHEME}://${ENDPOINT_HOSTNAME}:${ENDPOINT_PORT}"
```
The next steps use the endpoint to read the data in a python notebook

1. Insert a new notebook cell to install pandas and pyarrow packages:
  ```python
  %pip install pandas pyarrow==7.0.*
  ```
2. Insert a new notebook cell to read the data using the endpoint value extracted from the `FybrikApplication` in the previous step:
  ```bash
  import json
  import pyarrow.flight as fl
  import pandas as pd

  # Create a Flight client
  client = fl.connect('<ENDPOINT>')

  # Prepare the request
  request = {
      "asset": "fybrik-notebook-sample/<CATALOGED_ASSET>",
      # To request specific columns add to the request a "columns" key with a list of column names
      # "columns": [...]
  }

  # Send request and fetch result as a pandas DataFrame
  info = client.get_flight_info(fl.FlightDescriptor.for_command(json.dumps(request)))
  reader: fl.FlightStreamReader = client.do_get(info.endpoints[0].ticket)
  df_read: pd.DataFrame = reader.read_pandas()
  ```
4. Insert a new notebook cell with the following command to visualize the result:
  ```
  df_read
  ```
5. Execute all notebook cells and notice that the `Name` and `Email` columns appear redacted.


## Cleanup

When you’re finished experimenting with the notebook sample, clean it up:
1. Stop `kubectl port-forward` processes (e.g., using `pkill kubectl`)
1. Delete the namespace created for this sample:
    ```bash
    kubectl delete namespace fybrik-notebook-sample
    ```
1. Delete the policy created on fybrik-system namespace:
    ```bash
    kubectl -n fybrik-system delete configmap sample-policy-read
    ```
